{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSNA 2024 Lumbar Spine Degenerative Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter Notebook for Pytorch and Deep learning techniques\n",
    "\n",
    "Using ResNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this notebook contains?\n",
    "\n",
    "* Data organized in an understandable and easy to use way\n",
    "* A pretrained ResNET for inference\n",
    "\n",
    "I have tried creating a notebook where you can just plug your deep learning models and everything else is sorted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pydicom as dicom\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "import pandas as pd\n",
    "\n",
    "import pydicom as dicom # dicom\n",
    "import pydicom\n",
    "from pydicom.pixel_data_handlers.util import apply_voi_lut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "DATA_DIR = 'data/'\n",
    "#DATA_DIR = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\n",
    "\n",
    "train  = pd.read_csv(DATA_DIR + 'train.csv')\n",
    "label = pd.read_csv(DATA_DIR + 'train_label_coordinates.csv')\n",
    "train_desc  = pd.read_csv(DATA_DIR + 'train_series_descriptions.csv')\n",
    "test_desc   = pd.read_csv(DATA_DIR + 'test_series_descriptions.csv')\n",
    "sub         = pd.read_csv(DATA_DIR + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate image paths based on directory structure\n",
    "def generate_image_paths(df, data_dir):\n",
    "    image_paths = []\n",
    "    for study_id, series_id in zip(df['study_id'], df['series_id']):\n",
    "        study_dir = os.path.join(data_dir, str(study_id))\n",
    "        series_dir = os.path.join(study_dir, str(series_id))\n",
    "        images = os.listdir(series_dir)\n",
    "        image_paths.extend([os.path.join(series_dir, img) for img in images])\n",
    "    return image_paths\n",
    "\n",
    "# Generate image paths for train and test data\n",
    "train_image_paths = generate_image_paths(train_desc, f'{DATA_DIR}/train_images')\n",
    "test_image_paths = generate_image_paths(test_desc, f'{DATA_DIR}/test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to open and display DICOM images\n",
    "def display_dicom_images(image_paths):\n",
    "    plt.figure(figsize=(15, 5))  # Adjust figure size if needed\n",
    "    for i, path in enumerate(image_paths[:3]):\n",
    "        ds = pydicom.dcmread(path)\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.imshow(ds.pixel_array, cmap=plt.cm.bone)\n",
    "        plt.title(f\"Image {i+1}\")\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Display the first three DICOM images\n",
    "display_dicom_images(train_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Function to open and display DICOM images along with coordinates\n",
    "def display_dicom_with_coordinates(image_paths, label_df):\n",
    "    fig, axs = plt.subplots(1, len(image_paths), figsize=(18, 6))\n",
    "    \n",
    "    for idx, path in enumerate(image_paths):  # Display images\n",
    "        study_id = int(path.split('/')[-3])\n",
    "        series_id = int(path.split('/')[-2])\n",
    "        \n",
    "        # Filter label coordinates for the current study and series\n",
    "        filtered_labels = label_df[(label_df['study_id'] == study_id) & (label_df['series_id'] == series_id)]\n",
    "        \n",
    "        # Read DICOM image\n",
    "        ds = pydicom.dcmread(path)\n",
    "        \n",
    "        # Plot DICOM image\n",
    "        axs[idx].imshow(ds.pixel_array, cmap='gray')\n",
    "        axs[idx].set_title(f\"Study ID: {study_id}, Series ID: {series_id}\")\n",
    "        axs[idx].axis('off')\n",
    "        \n",
    "        # Plot coordinates\n",
    "        for _, row in filtered_labels.iterrows():\n",
    "            axs[idx].plot(row['x'], row['y'], 'ro', markersize=5)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load DICOM files from a folder\n",
    "def load_dicom_files(path_to_folder):\n",
    "    files = [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith('.dcm')]\n",
    "    files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0].split('-')[-1]))\n",
    "    return files\n",
    "\n",
    "# Display DICOM images with coordinates\n",
    "study_id = \"100206310\"\n",
    "study_folder = f'{DATA_DIR}/train_images/{study_id}'\n",
    "\n",
    "image_paths = []\n",
    "for series_folder in os.listdir(study_folder):\n",
    "    series_folder_path = os.path.join(study_folder, series_folder)\n",
    "    dicom_files = load_dicom_files(series_folder_path)\n",
    "    if dicom_files:\n",
    "        image_paths.append(dicom_files[0])  # Add the first image from each series\n",
    "\n",
    "\n",
    "display_dicom_with_coordinates(image_paths, label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to reshape a single row of the DataFrame\n",
    "def reshape_row(row):\n",
    "    data = {'study_id': [], 'condition': [], 'level': [], 'severity': []}\n",
    "    \n",
    "    for column, value in row.items():\n",
    "        if column not in ['study_id', 'series_id', 'instance_number', 'x', 'y', 'series_description']:\n",
    "            parts = column.split('_')\n",
    "            condition = ' '.join([word.capitalize() for word in parts[:-2]])\n",
    "            level = parts[-2].capitalize() + '/' + parts[-1].capitalize()\n",
    "            data['study_id'].append(row['study_id'])\n",
    "            data['condition'].append(condition)\n",
    "            data['level'].append(level)\n",
    "            data['severity'].append(value)\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Reshape the DataFrame for all rows\n",
    "new_train_df = pd.concat([reshape_row(row) for _, row in train.iterrows()], ignore_index=True)\n",
    "\n",
    "# Display the first few rows of the reshaped dataframe\n",
    "new_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print columns in a neat way\n",
    "print(\"\\nColumns in new_train_df:\")\n",
    "print(\",\".join(new_train_df.columns))\n",
    "\n",
    "print(\"\\nColumns in label:\")\n",
    "print(\",\".join(label.columns))\n",
    "\n",
    "print(\"\\nColumns in test_desc:\")\n",
    "print(\",\".join(test_desc.columns))\n",
    "\n",
    "print(\"\\nColumns in sub:\")\n",
    "print(\",\".join(sub.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on the common columns\n",
    "merged_df = pd.merge(new_train_df, label, on=['study_id', 'condition', 'level'], how='inner')\n",
    "# Merge the dataframes on the common column 'series_id'\n",
    "final_merged_df = pd.merge(merged_df, train_desc, on='series_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on the common column 'series_id'\n",
    "final_merged_df = pd.merge(merged_df, train_desc, on=['series_id','study_id'], how='inner')\n",
    "# Display the first few rows of the final merged dataframe\n",
    "final_merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df[final_merged_df['study_id'] == 100206310].sort_values(['x','y'],ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df[final_merged_df['series_id'] == 1012284084].sort_values(\"instance_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see what the data represents\n",
    "\n",
    "Series ID 1012284084 contains 60 images, and how each image maps to each level and condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe for the given study_id and sort by instance_number\n",
    "filtered_df = final_merged_df[final_merged_df['study_id'] == 1013589491].sort_values(\"instance_number\")\n",
    "\n",
    "# Display the resulting dataframe\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort final_merged_df by study_id, series_id, and series_description\n",
    "sorted_final_merged_df = final_merged_df[final_merged_df['study_id'] == 1013589491].sort_values(by=['series_id', 'series_description', 'instance_number'])\n",
    "sorted_final_merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that, <br>\n",
    "Saggital T1 images map to Neural Foraminal Narrowing <br>\n",
    "Axial T2 images map to Subarticular Stenosis <br>\n",
    "Saggital T2/STIR map to Canal Stenosis <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the row_id column\n",
    "final_merged_df['row_id'] = (\n",
    "    final_merged_df['study_id'].astype(str) + '_' +\n",
    "    final_merged_df['condition'].str.lower().str.replace(' ', '_') + '_' +\n",
    "    final_merged_df['level'].str.lower().str.replace('/', '_')\n",
    ")\n",
    "\n",
    "# Create the image_path column\n",
    "final_merged_df['image_path'] = (\n",
    "    f'{DATA_DIR}/train_images/' + \n",
    "    final_merged_df['study_id'].astype(str) + '/' +\n",
    "    final_merged_df['series_id'].astype(str) + '/' +\n",
    "    final_merged_df['instance_number'].astype(str) + '.dcm'\n",
    ")\n",
    "\n",
    "# Note: Check image path, since there's 1 instance id, for 1 image, but there's many more images other than the ones labelled in the instance ID. \n",
    "\n",
    "# Display the updated dataframe\n",
    "final_merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df[final_merged_df[\"severity\"] == \"Normal/Mild\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merged_df[final_merged_df[\"severity\"] == \"Moderate\"].value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base path for test images\n",
    "base_path = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/test_images/'\n",
    "\n",
    "# Function to get image paths for a series\n",
    "def get_image_paths(row):\n",
    "    series_path = os.path.join(base_path, str(row['study_id']), str(row['series_id']))\n",
    "    if os.path.exists(series_path):\n",
    "        return [os.path.join(series_path, f) for f in os.listdir(series_path) if os.path.isfile(os.path.join(series_path, f))]\n",
    "    return []\n",
    "\n",
    "# Mapping of series_description to conditions\n",
    "condition_mapping = {\n",
    "    'Sagittal T1': {'left': 'left_neural_foraminal_narrowing', 'right': 'right_neural_foraminal_narrowing'},\n",
    "    'Axial T2': {'left': 'left_subarticular_stenosis', 'right': 'right_subarticular_stenosis'},\n",
    "    'Sagittal T2/STIR': 'spinal_canal_stenosis'\n",
    "}\n",
    "\n",
    "# Create a list to store the expanded rows\n",
    "expanded_rows = []\n",
    "\n",
    "# Expand the dataframe by adding new rows for each file path\n",
    "for index, row in test_desc.iterrows():\n",
    "    image_paths = get_image_paths(row)\n",
    "    conditions = condition_mapping.get(row['series_description'], {})\n",
    "    if isinstance(conditions, str):  # Single condition\n",
    "        conditions = {'left': conditions, 'right': conditions}\n",
    "    for side, condition in conditions.items():\n",
    "        for image_path in image_paths:\n",
    "            expanded_rows.append({\n",
    "                'study_id': row['study_id'],\n",
    "                'series_id': row['series_id'],\n",
    "                'series_description': row['series_description'],\n",
    "                'image_path': image_path,\n",
    "                'condition': condition,\n",
    "                'row_id': f\"{row['study_id']}_{condition}\"\n",
    "            })\n",
    "\n",
    "# Create a new dataframe from the expanded rows\n",
    "expanded_test_desc = pd.DataFrame(expanded_rows)\n",
    "\n",
    "# Display the resulting dataframe\n",
    "expanded_test_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change severity column labels\n",
    "#Normal/Mild': 'normal_mild', 'Moderate': 'moderate', 'Severe': 'severe'}\n",
    "final_merged_df['severity'] = final_merged_df['severity'].map({'Normal/Mild': 'normal_mild', 'Moderate': 'moderate', 'Severe': 'severe'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = expanded_test_desc\n",
    "train_data = final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define a function to check if a path exists\n",
    "def check_exists(path):\n",
    "    return os.path.exists(path)\n",
    "\n",
    "# Define a function to check if a study ID directory exists\n",
    "def check_study_id(row):\n",
    "    study_id = row['study_id']\n",
    "    path = f'{DATA_DIR}/train_images/{study_id}'\n",
    "    return check_exists(path)\n",
    "\n",
    "# Define a function to check if a series ID directory exists\n",
    "def check_series_id(row):\n",
    "    study_id = row['study_id']\n",
    "    series_id = row['series_id']\n",
    "    path = f'{DATA_DIR}/train_images/{study_id}/{series_id}'\n",
    "    return check_exists(path)\n",
    "\n",
    "# Define a function to check if an image file exists\n",
    "def check_image_exists(row):\n",
    "    image_path = row['image_path']\n",
    "    return check_exists(image_path)\n",
    "\n",
    "# Apply the functions to the train_data dataframe\n",
    "train_data['study_id_exists'] = train_data.apply(check_study_id, axis=1)\n",
    "train_data['series_id_exists'] = train_data.apply(check_series_id, axis=1)\n",
    "train_data['image_exists'] = train_data.apply(check_image_exists, axis=1)\n",
    "\n",
    "# Filter train_data\n",
    "train_data = train_data[(train_data['study_id_exists']) & (train_data['series_id_exists']) & (train_data['image_exists'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dicom(path):\n",
    "    dicom = pydicom.filereader.dcmread(path)\n",
    "    data = dicom.pixel_array\n",
    "    data = data - np.min(data)\n",
    "    if np.max(data) != 0:\n",
    "        data = data / np.max(data)\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images randomly\n",
    "import random\n",
    "images = []\n",
    "row_ids = []\n",
    "selected_indices = random.sample(range(len(train_data)), 2)\n",
    "for i in selected_indices:\n",
    "    image = load_dicom(train_data['image_path'][i])\n",
    "    images.append(image)\n",
    "    row_ids.append(train_data['row_id'][i])\n",
    "\n",
    "# Plot images\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "for i in range(2):\n",
    "    ax[i].imshow(images[i], cmap='gray')\n",
    "    ax[i].set_title(f'Row ID: {row_ids[i]}', fontsize=8)\n",
    "    ax[i].axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for one hot encoding\n",
    "#train_data[['normal_mild', 'severe', 'moderate']] = train_data[['normal_mild', 'severe', 'moderate']].astype(int)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.dataframe['image_path'][index]\n",
    "        image = load_dicom(image_path)  # Define this function to load your DICOM images\n",
    "        label = self.dataframe['severity'][index]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Function to create datasets and dataloaders for each series description\n",
    "def create_datasets_and_loaders(df, series_description, transform, batch_size=8):\n",
    "    filtered_df = df[df['series_description'] == series_description]\n",
    "    \n",
    "    train_df, val_df = train_test_split(filtered_df, test_size=0.2, random_state=42)\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "\n",
    "    train_dataset = CustomDataset(train_df, transform)\n",
    "    val_dataset = CustomDataset(val_df, transform)\n",
    "\n",
    "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return trainloader, valloader, len(train_df), len(val_df)\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: (x * 255).astype(np.uint8)),  # Convert back to uint8 for PIL\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataloaders for each series description\n",
    "dataloaders = {}\n",
    "lengths = {}\n",
    "\n",
    "trainloader_t1, valloader_t1, len_train_t1, len_val_t1 = create_datasets_and_loaders(train_data, 'Sagittal T1', transform)\n",
    "trainloader_t2, valloader_t2, len_train_t2, len_val_t2 = create_datasets_and_loaders(train_data, 'Axial T2', transform)\n",
    "trainloader_t2stir, valloader_t2stir, len_train_t2stir, len_val_t2stir = create_datasets_and_loaders(train_data, 'Sagittal T2/STIR', transform)\n",
    "\n",
    "dataloaders['Sagittal T1'] = (trainloader_t1, valloader_t1)\n",
    "dataloaders['Axial T2'] = (trainloader_t2, valloader_t2)\n",
    "dataloaders['Sagittal T2/STIR'] = (trainloader_t2stir, valloader_t2stir)\n",
    "\n",
    "lengths['Sagittal T1'] = (len_train_t1, len_val_t1)\n",
    "lengths['Axial T2'] = (len_train_t2, len_val_t2)\n",
    "lengths['Sagittal T2/STIR'] = (len_train_t2stir, len_val_t2stir)\n",
    "\n",
    "# Dictionary mapping labels to indices\n",
    "label_map = {'Mild': 0, 'Moderate': 1, 'Severe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to visualize a batch of images\n",
    "def visualize_batch(dataloader):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    fig, axes = plt.subplots(1, len(images), figsize=(20, 5))\n",
    "    for i, (img, lbl) in enumerate(zip(images, labels)):\n",
    "        ax = axes[i]\n",
    "        img = img.permute(1, 2, 0)  # Convert to HWC for visualization\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Label: {lbl}\")\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from each dataloader\n",
    "print(\"Visualizing Sagittal T1 samples\")\n",
    "visualize_batch(trainloader_t1)\n",
    "print(\"Visualizing Axial T2 samples\")\n",
    "visualize_batch(trainloader_t2)\n",
    "print(\"Visualizing Sagittal T2/STIR samples\")\n",
    "visualize_batch(trainloader_t2stir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image, label = next(iter(trainloader_t2))\n",
    "sample = image[1].permute(1, 2, 0)  #sample\n",
    "\n",
    "# Plot images\n",
    "plt.figsize=(8, 4)\n",
    "plt.imshow(images[0], cmap='gray')\n",
    "plt.title(label[0])\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class CustomEfficientNetV2(nn.Module):\n",
    "    def __init__(self, num_classes=3, pretrained_weights=None):\n",
    "        super(CustomEfficientNetV2, self).__init__()\n",
    "        self.model = models.efficientnet_v2_s(weights=None)\n",
    "        if pretrained_weights:\n",
    "            self.model.load_state_dict(torch.load(pretrained_weights))\n",
    "        num_ftrs = self.model.classifier[-1].in_features\n",
    "        self.model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def unfreeze_model(self):\n",
    "        # Unfreeze the last 20 layers, keeping BatchNorm layers frozen\n",
    "        for layer in list(self.model.features.children())[-20:]:\n",
    "            if not isinstance(layer, nn.BatchNorm2d):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = True\n",
    "        \n",
    "        # Unfreeze the classifier\n",
    "        for param in self.model.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "# Path to the locally uploaded weights file\n",
    "weights_path = DATA_DIR + 'efficientnet_v2_s-dd5fe13b.pth'\n",
    "\n",
    "# Initialize models\n",
    "sagittal_t1_model = CustomEfficientNetV2(num_classes=3, pretrained_weights=weights_path).to(device)\n",
    "axial_t2_model = CustomEfficientNetV2(num_classes=3, pretrained_weights=weights_path).to(device)\n",
    "sagittal_t2stir_model = CustomEfficientNetV2(num_classes=3, pretrained_weights=weights_path).to(device)\n",
    "\n",
    "# Optionally freeze initial layers\n",
    "for param in sagittal_t1_model.model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in axial_t2_model.model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in sagittal_t2stir_model.model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the final fully connected layer\n",
    "for param in sagittal_t1_model.model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in axial_t2_model.model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "for param in sagittal_t2stir_model.model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Training parameters\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize separate optimizers for each model\n",
    "optimizer_sagittal_t1 = torch.optim.Adam(sagittal_t1_model.model.classifier.parameters(), lr=0.001)\n",
    "optimizer_axial_t2 = torch.optim.Adam(axial_t2_model.model.classifier.parameters(), lr=0.001)\n",
    "optimizer_sagittal_t2stir = torch.optim.Adam(sagittal_t2stir_model.model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "# Store the models and optimizers in dictionaries for easy access\n",
    "models = {\n",
    "    'Sagittal T1': sagittal_t1_model,\n",
    "    'Axial T2': axial_t2_model,\n",
    "    'Sagittal T2/STIR': sagittal_t2stir_model,\n",
    "}\n",
    "optimizers = {\n",
    "    'Sagittal T1': optimizer_sagittal_t1,\n",
    "    'Axial T2': optimizer_axial_t2,\n",
    "    'Sagittal T2/STIR': optimizer_sagittal_t2stir,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in sagittal_t1_model.parameters() if p.requires_grad)\n",
    "print(f\"Number of parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {'normal_mild': 0, 'moderate': 1, 'severe': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in trainloader_t2:\n",
    "    labels = torch.tensor([label_map[label] for label in labels])\n",
    "    labels = labels.to(device)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from copy import deepcopy\n",
    "\n",
    "def train_model(model, trainloader, valloader, len_train, len_val, optimizer, num_epochs=10, patience=3):\n",
    "    # Learning rate scheduler\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_model_wts = deepcopy(model.state_dict())\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct_train = 0\n",
    "        \n",
    "        with tqdm(trainloader, unit=\"batch\") as tepoch:\n",
    "            for images, labels in tepoch:\n",
    "                images, labels = images.to(device), torch.tensor([label_map[label] for label in labels]).to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(probabilities, 1)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                \n",
    "                tepoch.set_postfix(epoch=epoch+1)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss /= len(trainloader)\n",
    "        train_acc = 100 * correct_train / len_train\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss, correct_val = 0, 0\n",
    "        with torch.no_grad():\n",
    "            with tqdm(valloader, unit=\"batch\") as vepoch:\n",
    "                for images, labels in vepoch:\n",
    "                    images, labels = images.to(device), torch.tensor([label_map[label] for label in labels]).to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    probabilities = torch.softmax(outputs, dim=1).squeeze(0)\n",
    "                    _, predicted = torch.max(probabilities, 1)\n",
    "                    correct_val += (predicted == labels).sum().item()\n",
    "                    \n",
    "                    vepoch.set_postfix(epoch=epoch+1)\n",
    "        \n",
    "        val_loss /= len(valloader)\n",
    "        val_acc = 100 * correct_val / len_val\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save the best model and check for early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_wts = deepcopy(model.state_dict())\n",
    "            counter = 0\n",
    "            torch.save(best_model_wts, f'best_model_{epoch+1}.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "            break\n",
    "    \n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, best_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Training all models\n",
    "for desc, model in models.items():\n",
    "    if desc == 'Sagittal T1':\n",
    "        trainloader, valloader, len_train, len_val = trainloader_t1, valloader_t1, len_train_t1, len_val_t1\n",
    "    elif desc == 'Axial T2':\n",
    "        trainloader, valloader, len_train, len_val = trainloader_t2, valloader_t2, len_train_t2, len_val_t2\n",
    "    elif desc == 'Sagittal T2/STIR':\n",
    "        trainloader, valloader, len_train, len_val = trainloader_t2stir, valloader_t2stir, len_train_t2stir, len_val_t2stir\n",
    "    \n",
    "    print(f\"Training model for {desc}\")\n",
    "    train_model(model, trainloader, valloader, len_train, len_val, optimizers[desc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "train_data['level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "expanded_test_desc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "levels = ['l1_l2', 'l2_l3', 'l3_l4', 'l4_l5', 'l5_s1']\n",
    "\n",
    "# Function to update row_id with levels\n",
    "def update_row_id(row, levels):\n",
    "    level = levels[row.name % len(levels)]\n",
    "    return f\"{row['study_id']}_{row['condition']}_{level}\"\n",
    "\n",
    "# Update row_id in expanded_test_desc to include levels\n",
    "expanded_test_desc['row_id'] = expanded_test_desc.apply(lambda row: update_row_id(row, levels), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "expanded_test_desc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Define a custom test dataset class\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.dataframe['image_path'][index]\n",
    "        image = load_dicom(image_path)  # Define this function to load your DICOM images\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Define the transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create a test dataset and dataloader\n",
    "test_dataset = TestDataset(expanded_test_desc, transform)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "for image in testloader:\n",
    "    print(image.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Function to get the model based on series_description\n",
    "def get_model(series_description):\n",
    "    return models.get(series_description, None)\n",
    "\n",
    "# Function to make predictions on the test data\n",
    "def predict_test_data(testloader, expanded_test_desc):\n",
    "    predictions = []\n",
    "    normal_mild_probs = []\n",
    "    moderate_probs = []\n",
    "    severe_probs = []\n",
    "    \n",
    "    for model in models.values():\n",
    "        model.eval()\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for idx, images in enumerate(tqdm(testloader)):\n",
    "            images = images.to(device)\n",
    "            series_description = expanded_test_desc.iloc[idx]['series_description']\n",
    "            model = get_model(series_description)\n",
    "            model = get_model(series_description)\n",
    "            if model:\n",
    "                model.eval()  # Set the model to eval mode\n",
    "                outputs = model(images)\n",
    "                probs = torch.softmax(outputs, dim=1).squeeze(0)\n",
    "                normal_mild_probs.append(probs[0].item())\n",
    "                moderate_probs.append(probs[1].item())\n",
    "                severe_probs.append(probs[2].item())\n",
    "                predictions.append(probs)\n",
    "            else:\n",
    "                normal_mild_probs.append(None)\n",
    "                moderate_probs.append(None)\n",
    "                severe_probs.append(None)\n",
    "                predictions.append(None)\n",
    "    return normal_mild_probs, moderate_probs, severe_probs, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "normal_mild_probs, moderate_probs, severe_probs, test_predictions = predict_test_data(testloader, expanded_test_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "test_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Add predictions and probabilities to the test DataFrame\n",
    "expanded_test_desc['normal_mild'] = normal_mild_probs\n",
    "expanded_test_desc['moderate'] = moderate_probs\n",
    "expanded_test_desc['severe'] = severe_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "submission = expanded_test_desc[[\"row_id\",\"normal_mild\",\"moderate\",\"severe\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "submission.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Group by 'row_id' and sum the values\n",
    "grouped_submission = submission.groupby('row_id').max().reset_index()\n",
    "\n",
    "# Normalize the columns\n",
    "#grouped_submission[['normal_mild', 'moderate', 'severe']] = grouped_submission[['normal_mild', 'moderate', 'severe']].div(grouped_submission[['normal_mild', 'moderate', 'severe']].sum(axis=1), axis=0)\n",
    "\n",
    "# Check the first 3 rows\n",
    "grouped_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "len(grouped_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sub[['normal_mild', 'moderate', 'severe']] = grouped_submission[['normal_mild', 'moderate', 'severe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Save the DataFrame to \"submission.csv\" in the desired directory\n",
    "sub.to_csv(\"/kaggle/working/submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8561470,
     "sourceId": 71549,
     "sourceType": "competition"
    },
    {
     "modelId": 2797,
     "modelInstanceId": 4608,
     "sourceId": 6158,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 74163,
     "modelInstanceId": 54020,
     "sourceId": 64765,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 74163,
     "modelInstanceId": 54048,
     "sourceId": 64795,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
